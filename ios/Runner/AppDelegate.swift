import Flutter
import UIKit
import MLKitFaceDetection
import MLKitVision
import AVFoundation

@main
@objc class AppDelegate: FlutterAppDelegate {
  private var faceDetector: FaceDetector?
  private let detectionQueue = DispatchQueue(label: "com.facepixel.facedetection", qos: .userInitiated)

  override func application(
    _ application: UIApplication,
    didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?
  ) -> Bool {
    NSLog("ðŸŽ AppDelegate: application:didFinishLaunchingWithOptions called")

    guard let controller = window?.rootViewController as? FlutterViewController else {
      NSLog("âŒ AppDelegate: Failed to get FlutterViewController")
      return super.application(application, didFinishLaunchingWithOptions: launchOptions)
    }

    NSLog("âœ… AppDelegate: Got FlutterViewController")

    let faceDetectionChannel = FlutterMethodChannel(
      name: "com.facepixel.app/faceDetection",
      binaryMessenger: controller.binaryMessenger
    )

    faceDetectionChannel.setMethodCallHandler { (call: FlutterMethodCall, result: @escaping FlutterResult) in
      NSLog("ðŸ“± AppDelegate: Method channel called: \(call.method)")
      switch call.method {
      case "initializeFaceDetection":
        NSLog("ðŸ”§ AppDelegate: Initializing face detection")
        self.initializeFaceDetection(result: result)
      case "processFrame":
        NSLog("ðŸ“· AppDelegate: Processing frame")
        self.processFrame(call: call, result: result)
      case "cleanupCamera":
        NSLog("ðŸ§¹ AppDelegate: Cleaning up camera resources")
        self.cleanupCamera(result: result)
      default:
        NSLog("âš ï¸ AppDelegate: Unknown method: \(call.method)")
        result(FlutterMethodNotImplemented)
      }
    }

    GeneratedPluginRegistrant.register(with: self)
    NSLog("âœ… AppDelegate: Plugins registered successfully")
    return super.application(application, didFinishLaunchingWithOptions: launchOptions)
  }

  private func initializeFaceDetection(result: @escaping FlutterResult) {
    NSLog("ðŸ”§ initializeFaceDetection: Starting on main thread")
    let options = FaceDetectorOptions()
    options.performanceMode = .fast
    options.landmarkMode = .none
    options.classificationMode = .none
    options.minFaceSize = CGFloat(0.01)

    faceDetector = FaceDetector.faceDetector(options: options)
    NSLog("âœ… initializeFaceDetection: FaceDetector created successfully")
    result(true)
  }

  private func processFrame(call: FlutterMethodCall, result: @escaping FlutterResult) {
    NSLog("ðŸ“· processFrame: Starting")
    guard let args = call.arguments as? [String: Any],
          let frameBytes = args["frameBytes"] as? FlutterStandardTypedData,
          let width = args["width"] as? Int,
          let height = args["height"] as? Int,
          let rotation = args["rotation"] as? Int else {
      NSLog("âŒ processFrame: Failed to parse arguments")
      result(["success": false, "faces": []])
      return
    }

    guard let detector = faceDetector else {
      NSLog("âš ï¸ processFrame: Face detector not available (may be during camera switch)")
      result(["success": false, "faces": []])
      return
    }

    let imageData = frameBytes.data
    NSLog("ðŸ“· processFrame: Received frame \(width)x\(height), rotation: \(rotation)Â°, data size: \(imageData.count)")

    // Use serial queue to process frames sequentially
    detectionQueue.async { [weak self] in
      NSLog("ðŸ“· processFrame: Running on background thread")

      do {
        // FOLLOW ANDROID PATTERN: Keep frame data alive, create image, detect faces
        // Keep frame data in memory - critical for CVPixelBuffer to access it safely
        let frameDataHolder = NSData(bytes: (imageData as NSData).bytes, length: imageData.count)

        // Auto-detect pixel format - same logic as Android
        let expectedBGRASize = width * height * 4
        let expectedYUVSize = width * height * 3 / 2

        let pixelFormat: OSType
        let bytesPerRow: Int

        if imageData.count == expectedBGRASize {
          NSLog("ðŸ“· processFrame: BGRA8888 format, width=\(width) height=\(height)")
          pixelFormat = kCVPixelFormatType_32BGRA
          bytesPerRow = width * 4
        } else if imageData.count >= expectedYUVSize && imageData.count <= expectedYUVSize + 100 {
          NSLog("ðŸ“· processFrame: YUV420 format, width=\(width) height=\(height)")
          pixelFormat = kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
          bytesPerRow = width
        } else {
          NSLog("âŒ processFrame: Unknown format, data size: \(imageData.count), expected BGRA: \(expectedBGRASize), YUV: \(expectedYUVSize)")
          DispatchQueue.main.async {
            result(["success": false, "faces": []])
          }
          return
        }

        // Create CVPixelBuffer from frame data pointer (frameDataHolder keeps data alive)
        var pixelBuffer: CVPixelBuffer?
        let options: [String: Any] = [
          kCVPixelBufferCGImageCompatibilityKey as String: true,
          kCVPixelBufferCGBitmapContextCompatibilityKey as String: true
        ]

        let status = CVPixelBufferCreateWithBytes(
          kCFAllocatorDefault,
          width,
          height,
          pixelFormat,
          UnsafeMutableRawPointer(mutating: frameDataHolder.bytes),
          bytesPerRow,
          nil,
          nil,
          options as CFDictionary,
          &pixelBuffer
        )

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
          NSLog("âŒ processFrame: Failed to create CVPixelBuffer, status: \(status)")
          DispatchQueue.main.async {
            result(["success": false, "faces": []])
          }
          return
        }

        NSLog("âœ… processFrame: CVPixelBuffer created successfully")

        // Create VisionImage directly from CVPixelBuffer (simpler than CMSampleBuffer path)
        let visionImage = VisionImage(buffer: buffer)
        visionImage.orientation = self?.getImageOrientation(from: rotation) ?? .up

        NSLog("ðŸ“ processFrame: Running SYNCHRONOUS face detection on background thread")
        let faces = try detector.results(in: visionImage)
        NSLog("âœ… processFrame: Face detection completed, found \(faces.count) faces")

        var faceArray: [[String: NSNumber]] = []

        for (index, face) in faces.enumerated() {
          let boundingBox = face.frame
          NSLog("ðŸ“ Face \(index): x=\(boundingBox.origin.x) y=\(boundingBox.origin.y) w=\(boundingBox.width) h=\(boundingBox.height)")

          // Only include meaningfully visible faces
          if boundingBox.width >= 20 && boundingBox.height >= 20 {
            faceArray.append([
              "x": NSNumber(value: Float(boundingBox.origin.x)),
              "y": NSNumber(value: Float(boundingBox.origin.y)),
              "width": NSNumber(value: Float(boundingBox.width)),
              "height": NSNumber(value: Float(boundingBox.height))
            ])
          }
        }

        DispatchQueue.main.async {
          NSLog("ðŸ“² processFrame: Returning \(faceArray.count) faces to Flutter")
          result(["success": true, "faces": faceArray])
        }

      } catch {
        NSLog("âŒ processFrame: Face detection error - \(error.localizedDescription)")
        DispatchQueue.main.async {
          result(["success": false, "faces": []])
        }
      }
    }
  }

  private func getImageOrientation(from rotation: Int) -> UIImage.Orientation {
    switch rotation {
    case 0:
      return .up
    case 90:
      return .right
    case 180:
      return .down
    case 270:
      return .left
    default:
      return .up
    }
  }

  private func cleanupCamera(result: @escaping FlutterResult) {
    NSLog("ðŸ§¹ cleanupCamera: Releasing face detector")
    self.faceDetector = nil
    result(true)
  }
}
